import requests
from bs4 import BeautifulSoup
import json
import time

def scrape_category(url, pages=7, limit=200):
    products = []
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept-Language": "it-IT,it;q=0.9"
    }

    for page in range(1, pages + 1):
        paged_url = f"{url}&page={page}"
        print("Scraping:", paged_url)

        r = requests.get(paged_url, headers=headers)
        soup = BeautifulSoup(r.text, "html.parser")

        items = soup.select(".s-result-item[data-asin]")
        for item in items:
            asin = item.get("data-asin")
            if not asin:
                continue

            # Prezzo
            price_el = item.select_one(".a-price .a-offscreen")
            if not price_el:
                continue

            price_text = price_el.get_text(strip=True)
            price = float(price_text.replace("â‚¬", "").replace(",", ".").strip())

            products.append({
                "asin": asin,
                "price": price
            })

            if len(products) >= limit:
                return products

        time.sleep(1)

    return products


CATEGORIES = {
    "alimentatori": "https://www.amazon.it/s?k=alimentatore+pc",
    "gpu": "https://www.amazon.it/s?k=scheda+video",
    "cpu": "https://www.amazon.it/s?k=cpu+intel+amd",
    "dissipatori": "https://www.amazon.it/s?k=dissipatore+cpu",
    "mobo": "https://www.amazon.it/s?k=scheda+madre",
    "case": "https://www.amazon.it/s?k=case+pc",
    "ram": "https://www.amazon.it/s?k=ram+ddr5",
    "ventole": "https://www.amazon.it/s?k=ventole+pc"
}

new_prices = {}

for name, url in CATEGORIES.items():
    print(f"\n--- Scraping categoria: {name} ---")
    products = scrape_category(url)

    for p in products:
        new_prices[p["asin"]] = p["price"]

with open("prices.json", "w") as f:
    json.dump(new_prices, f, indent=4)

print("\nprices.json generato con successo!")
